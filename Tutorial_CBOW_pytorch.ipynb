{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([2, 2, 3])\n",
      "sum \n",
      "  8  10  12\n",
      " 14  16  18\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      " 0.2931 -0.8245  0.3434\n",
      " 0.4421 -1.7660 -0.1732\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "        [[1,2,3],[4,5,6]],\n",
    "        [[7,8,9],[10,11,12]]\n",
    "    ]\n",
    "d = torch.Tensor(d)\n",
    "print(\"shape:\",d.size())\n",
    "\n",
    "print(\"sum\", d[0]+d[1])\n",
    "\n",
    "print(torch.randn([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  2   4   6\n",
      "  8  10  12\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Variable containing:\n",
      " 0.5434  1.2296 -0.0942  1.9574 -0.1271  1.7451\n",
      "-0.2989  1.3809  1.2106  1.9495  0.4360  0.1652\n",
      " 0.3684  0.9500 -0.0401 -0.3565 -0.6574 -2.0002\n",
      " 0.6157  0.3171  1.5491 -1.6937  1.9789  0.9560\n",
      "-1.0528  1.3535  0.3907  1.8049  0.3101  0.8006\n",
      "[torch.FloatTensor of size 5x6]\n",
      "\n",
      "Variable containing:\n",
      " 0.5434  1.2296  0.0000  1.9574  0.0000  1.7451\n",
      " 0.0000  1.3809  1.2106  1.9495  0.4360  0.1652\n",
      " 0.3684  0.9500  0.0000  0.0000  0.0000  0.0000\n",
      " 0.6157  0.3171  1.5491  0.0000  1.9789  0.9560\n",
      " 0.0000  1.3535  0.3907  1.8049  0.3101  0.8006\n",
      "[torch.FloatTensor of size 5x6]\n",
      "\n",
      "Variable containing:\n",
      " 0.0872  0.1733  0.0461  0.3587  0.0446  0.2901\n",
      " 0.0416  0.2232  0.1882  0.3941  0.0868  0.0662\n",
      " 0.2278  0.4075  0.1514  0.1103  0.0817  0.0213\n",
      " 0.1031  0.0765  0.2622  0.0102  0.4030  0.1449\n",
      " 0.0227  0.2519  0.0962  0.3956  0.0887  0.1449\n",
      "[torch.FloatTensor of size 5x6]\n",
      "\n",
      "Variable containing:\n",
      " 5\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Autograd genera la computation graph que calcula los backpropagation gradient eficientemente. Una computation graph especifica\n",
    "# como se combinan los datos para dar el output, y como la graph especifica los parametros involucrados en las operaciones tiene\n",
    "# todos los datos para calcular las derivadas. Se usa la clase autograd.Variable\n",
    "first = autograd.Variable(data=torch.Tensor([[1,2,3],[4,5,6]]), requires_grad=True)\n",
    "first = first * 2\n",
    "print(first.data)\n",
    "\n",
    "aux = autograd.Variable(data=torch.randn([5,6]))\n",
    "print(aux)\n",
    "print(F.relu(aux))\n",
    "\n",
    "print(F.softmax(aux))\n",
    "print(F.softmax(aux).sum())# debe devolver 1, ya que softmax pasa la matrix a probabilidades y la suma debe ser 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "data = [ (\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "         (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "         (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "         (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\") ]\n",
    "\n",
    "test_data = [ (\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "              (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "word_to_index = {}\n",
    "for frase, idioma in data + test_data:\n",
    "    for palabra in frase:\n",
    "        if palabra not in word_to_index:\n",
    "            word_to_index[palabra] = len(word_to_index)\n",
    "print(word_to_index) # Imprimimos la bag of words\n",
    "\n",
    "VOCAB_SIZE = len(word_to_index)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: https://github.com/rguthrie3/DeepLearningForNLPInPytorch\n",
    "class BagsOfWordsClassifier(nn.Module): # hereda de nn.module - contenedor basico de neural networks\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BagsOfWordsClassifier, self).__init__()\n",
    "        \n",
    "        #Linear proporciona el affine map, aplica una transformacion lineal a los datos, y = Ax + b, matrix A y vectores x y b\n",
    "        #Parametros a aprender son A y b (se le suele llamar bias)\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        \"\"\"\n",
    "    In the forward pass we receive a Tensor containing the input and return a\n",
    "    Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "    backward pass using the save_for_backward method.\n",
    "        \"\"\"\n",
    "        # Pasamos los valores por la funcion lineal, y luego por la no lineal para devolver probabilidades\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "    \n",
    "        #No hay backward xk cogemos el predefinido, pero si lo quisieramos hacer:\n",
    "        \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_index):\n",
    "    vec = torch.zeros(len(word_to_index))\n",
    "    for palabra in sentence:\n",
    "        vec[word_to_index[palabra]] += 1\n",
    "    return vec.view(1,-1)\n",
    "\n",
    "def make_target(label, label_to_index):\n",
    "    return torch.LongTensor([label_to_index[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0303 -0.0996 -0.1653 -0.0097 -0.1270  0.1892  0.0897 -0.0550 -0.0498  0.1454\n",
      " 0.1961  0.0792 -0.1286  0.0566  0.0367 -0.0973  0.1507  0.1139 -0.1598  0.1311\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.0269  0.0622 -0.0011 -0.1049 -0.1298 -0.1307 -0.1113  0.1915 -0.1748  0.0766\n",
      "-0.0178 -0.1017 -0.1327  0.1619  0.0461 -0.0503 -0.0724 -0.0724 -0.1491  0.0447\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.0886  0.0763 -0.0632  0.1349  0.0955 -0.1251\n",
      " 0.0622  0.0522 -0.0727  0.1604 -0.1952 -0.1476\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "\n",
      " 0.1611\n",
      " 0.0981\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BagsOfWordsClassifier(NUM_LABELS,VOCAB_SIZE)\n",
    "for param in model.parameters(): # Los parametros mostrados son A(matriz) y b(vector) \n",
    "    print(param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.8673 -0.5449\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_index)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print (log_probs)\n",
    " \n",
    "# El print devuelve un par de numeros (español o ingles) pero cada numero de que es?\n",
    "# Definimos el 1er indice como español, y el 2do como ingles\n",
    "label_to_index = { \"SPANISH\":0, \"ENGLISH\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.4805 -0.9635\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.8686 -0.5439\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.6949\n",
      " -1.7846\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Opcional, test data antes del entrenamiento, para ver el antes y el despues\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_index))\n",
    "    log_probs = model(bow_vec)\n",
    "    print (log_probs)\n",
    "print( next(model.parameters())[:,word_to_index[\"creo\"]] )# Print the matrix column corresponding to \"creo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Entrenamos, para ello calculamos la probabilidad como antes, calculamos la funcion de perdida y el gradiente de la misma.\n",
    "#Por ultimo actualizamos los parametros con un paso de gradiente. \n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "#Normalmente se hace el entrenamiento varias veces(5 a 30 veces)\n",
    "#100 es demasiado, pero los datasets reales tb tienen mas de 2 instancias\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # 1. Borrar contenido de los gradientes anteriores antes de cada instancia, debido a que pytorch los acumula.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 2. Crear vector BOW, y el target debe encapsularse en una Variable como un entero, asi que si vemos SPANISH sera 0\n",
    "        # De esta manera la fx de perdida sabe que el elemento 0 es probabilidad de Spanish\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_index), requires_grad=False)\n",
    "        target = autograd.Variable(make_target(label, label_to_index), requires_grad=False)\n",
    "        \n",
    "        # 3. Ejecutar el forward pass\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # 4. Calcular la perdida, gradientes, y actualizar los parametros llamando a optimizer.step\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1010 -2.3425\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.5055 -0.0852\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.7142\n",
      " 0.5046\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance,word_to_index))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "print(next(model.parameters())[:,word_to_index[\"to\"]])\n",
    "#Vemos como el 1er indice, el correspondiente a SPANISH sube mas que el de ENGLISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
